{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  rating  timestamp\n",
      "0      196      242       3  881250949\n",
      "1      186      302       3  891717742\n",
      "2       22      377       1  878887116\n",
      "3      244       51       2  880606923\n",
      "4      166      346       1  886397596\n"
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "# import pandas as pd\n",
    "\n",
    "# # URL for MovieLens 1M dataset (you can change the URL for other versions)\n",
    "# url = '../data/ml-1m/ratings.dat'\n",
    "\n",
    "# # Load the dataset directly into a pandas DataFrame\n",
    "# df = pd.read_csv(url, sep='::', header=None, names=['user_id', 'item_id', 'rating', 'timestamp'], engine='python')\n",
    "# # Display the first few rows to confirm the dataset loaded correctly\n",
    "# print(df.head())\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "\n",
    "# URL for MovieLens 100k dataset (you can change the URL for other versions)\n",
    "url = 'https://files.grouplens.org/datasets/movielens/ml-100k/u.data'\n",
    "\n",
    "# Load the dataset directly into a pandas DataFrame\n",
    "df = pd.read_csv(url, sep='\\t', header=None, names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  rating  timestamp\n",
      "0        0        0       3  881250949\n",
      "1        1        1       3  891717742\n",
      "2        2        2       1  878887116\n",
      "3        3        3       2  880606923\n",
      "4        4        4       1  886397596\n"
     ]
    }
   ],
   "source": [
    "# Take the unique value and mapping it\n",
    "user_mapping = {user_id: idx for idx, user_id in enumerate(df['user_id'].unique())} \n",
    "item_mapping = {item_id: idx for idx, item_id in enumerate(df['item_id'].unique())}\n",
    "\n",
    "# Applying the mappings\n",
    "df['user_id'] = df['user_id'].map(user_mapping)\n",
    "df['item_id'] = df['item_id'].map(item_mapping)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BPRDataset import BPRDataset\n",
    "\n",
    "# Convert from DataFrame into PyTorch tensor\n",
    "user_tensor = torch.tensor(df['user_id'].values,dtype=torch.long)\n",
    "item_tensor = torch.tensor(df['item_id'].values,dtype=torch.long)\n",
    "rating_tensor = torch.tensor(df['rating'].values,dtype=torch.float)\n",
    "\n",
    "num_items = df['item_id'].max() + 1\n",
    "\n",
    "# Create a dataset\n",
    "dataset = BPRDataset(df, num_items= num_items, min_rating=2.0)\n",
    "\n",
    "# Split the dataset into 70% training, 15% validation, and 15% test\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create a dataLoader for training, validation, and testing\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4842, Val Loss: 0.3268, Time: 7.59s\n",
      "Epoch 2/10, Train Loss: 0.2971, Val Loss: 0.2881, Time: 7.33s\n",
      "Epoch 3/10, Train Loss: 0.2564, Val Loss: 0.2521, Time: 7.86s\n",
      "Epoch 4/10, Train Loss: 0.2262, Val Loss: 0.2312, Time: 8.49s\n",
      "Epoch 5/10, Train Loss: 0.2069, Val Loss: 0.2225, Time: 7.84s\n",
      "Epoch 6/10, Train Loss: 0.1975, Val Loss: 0.2195, Time: 8.74s\n",
      "Epoch 7/10, Train Loss: 0.1881, Val Loss: 0.2136, Time: 7.99s\n",
      "Epoch 8/10, Train Loss: 0.1746, Val Loss: 0.2083, Time: 8.97s\n",
      "Epoch 9/10, Train Loss: 0.1663, Val Loss: 0.2028, Time: 8.57s\n",
      "Epoch 10/10, Train Loss: 0.1594, Val Loss: 0.1983, Time: 9.85s\n"
     ]
    }
   ],
   "source": [
    "from bpr_mf import MF\n",
    "\n",
    "model = MF(num_users=len(user_mapping)\n",
    "           , num_items= len(item_mapping)\n",
    "           , num_factors=30\n",
    "           , device=device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_losses, val_losses = model.train_model_bpr(train_loader, val_loader,\n",
    "                                         num_epochs=10,num_step=10,lr=0.002, reg=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate by AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(739, 1225, [771, 262, 364, 595, 662, 228, 767, 957, 56, 1511, 1229, 1263, 1254, 983, 1353, 807, 1371, 559, 1677, 425, 605, 444, 712, 776, 1192, 803, 466, 273, 250, 641, 1469, 4, 624, 40, 78, 32, 1669, 488, 77, 137, 426, 33, 487, 569, 1649, 809, 1359, 1115, 892, 346, 1558, 162, 1671, 1007, 567, 658, 569, 936, 1064, 1304, 1094, 752, 1216, 1543, 1208, 89, 1539, 641, 196, 461, 1197, 1044, 1348, 1678, 95, 177, 1183, 1109, 1032, 113, 793, 1442, 1617, 203, 1195, 274, 124, 468, 19, 715, 370, 1157, 216, 267, 1108, 267, 1109, 744, 935, 437]), (713, 420, [558, 159, 1064, 1338, 946, 256, 149, 1048, 289, 20, 325, 660, 368, 167, 486, 95, 800, 960, 55, 1052, 301, 190, 1622, 1303, 1134, 102, 40, 195, 1645, 1065, 381, 711, 37, 1337, 708, 1631, 1549, 953, 944, 935, 1421, 994, 1368, 598, 1563, 1217, 456, 865, 598, 632, 340, 490, 1213, 804, 474, 656, 1656, 232, 1651, 1514, 1119, 803, 145, 645, 1256, 488, 858, 508, 1157, 725, 798, 1117, 443, 1183, 503, 1088, 97, 1453, 482, 534, 820, 532, 807, 761, 1441, 973, 287, 115, 16, 1459, 1032, 1476, 995, 1513, 721, 81, 1308, 1107, 1597, 267]), (140, 432, [213, 919, 76, 502, 881, 232, 354, 1253, 1492, 1655, 1316, 1245, 783, 1318, 451, 1209, 1101, 538, 1347, 742, 1332, 1138, 526, 1138, 1180, 696, 804, 1167, 437, 1207, 1157, 1619, 1638, 684, 491, 851, 901, 139, 854, 1657, 1614, 1230, 406, 1060, 1055, 1115, 248, 801, 621, 1152, 802, 1027, 420, 496, 47, 1410, 162, 511, 1415, 420, 58, 841, 323, 1368, 1343, 653, 1028, 1040, 561, 440, 268, 971, 60, 956, 1466, 789, 1306, 1596, 834, 1396, 895, 984, 1424, 1046, 130, 998, 1010, 499, 1124, 1145, 616, 1514, 361, 1043, 383, 1663, 863, 611, 1499, 1573])]\n",
      "0.8908366141732283\n"
     ]
    }
   ],
   "source": [
    "from BPRDataset import BPREvalDataset\n",
    "from eval import evaluate_auc\n",
    "\n",
    "# Recover test DataFrame using the indices from random_split:\n",
    "test_df = df.iloc[test_data.indices]\n",
    "\n",
    "# Instantiate evaluation dataset with multiple negatives\n",
    "eval_dataset = BPREvalDataset(test_df, num_items=num_items, num_negatives=100, min_rating=2.0)\n",
    "\n",
    "# # Convert evaluation dataset into a list of tuples (user, pos_item, neg_item)\n",
    "eval_list = [\n",
    "    (user.item(), pos_item.item(), [neg.item() for neg in neg_items])\n",
    "    for user, pos_item, neg_items in eval_dataset\n",
    "]\n",
    "\n",
    "# First 3 tuples\n",
    "print(eval_list[:3])\n",
    "\n",
    "auc_score = evaluate_auc(model, eval_list)\n",
    "print(auc_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
