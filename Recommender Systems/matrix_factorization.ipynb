{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "Matrix factorization is a class of collaborative filtering models. Specifically, the model factorizes the user-item interaction matrix (e.g., rating matrix) into the product of two lower-rank matrices, capturing the low-rank structure of the user-item interactions.\n",
    "\n",
    "Let **R** ‚àà $\\mathbb{R}^{m\\times n}$ denote the interaction matrix with *m* users and *n* items, and the values of **R** represent explicit ratings. The user-item interaction will be factorized into a user latent matrix **P** ‚àà $\\mathbb{R}^{m\\times k}$ and an item latent matrix **Q** ‚àà $\\mathbb{R}^{n\\times k}$, where *k* ‚â™ *m, n*, is the latent factor size. Let **p_u** denote the *u*th row of **P** and **q_i** denote the *i*th row of **Q**. For a given item *i*, the elements of **q_i** measure the extent to which the item possesses those characteristics such as the genres and languages of a movie. For a given user *u*, the elements of **p_u** measure the extent of interest the user has in items‚Äô corresponding characteristics. These latent factors might measure obvious dimensions as mentioned in those examples or are completely uninterpretable. The predicted ratings can be estimated by\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{R}} = \\mathbf{P} \\mathbf{Q}^{\\top}\n",
    "$$ (21.3.1)\n",
    "\n",
    "where **RÃÇ** ‚àà $\\mathbb{R}^{m\\times n}$ is the predicted rating matrix which has the same shape as **R**. One major problem of this prediction rule is that users/items biases cannot be modeled. For example, some users tend to give higher ratings or some items always get lower ratings due to poorer quality. These biases are commonplace in real-world applications. To capture these biases, user-specific and item-specific bias terms are introduced. Specifically, the predicted rating user *u* gives to item *i* is calculated by\n",
    "\n",
    "$$\n",
    "\\hat{R}_{ui} = \\mathbf{p}_u \\mathbf{q}_i^{\\top} + b_u + b_i\n",
    "$$ (21.3.2)\n",
    "\n",
    "Then, we train the matrix factorization model by minimizing the mean squared error between predicted rating scores and real rating scores. The objective function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{\\mathbf{P}, \\mathbf{Q}, b} \\sum_{(u,i) \\in \\mathcal{K}} || R_{ui} - \\hat{R}_{ui} ||^2 + \\lambda ( ||\\mathbf{P}||_F^2 + ||\\mathbf{Q}||_F^2 + b_u^2 + b_i^2 )\n",
    "$$ (21.3.3)\n",
    "\n",
    "where *Œª* denotes the regularization rate. The regularizing term $\\lambda ( ||\\mathbf{P}||_F^2 + ||\\mathbf{Q}||_F^2 + b_u^2 + b_i^2 )$ is used to avoid over-fitting by penalizing the magnitude of the parameters. The (*u, i*) pairs for which **R_ui** is known are stored in the set ùí¶ = $\\{(u, i) | R_ui \\text{is known}\\}$. The model parameters can be learned with an optimization algorithm, such as Stochastic Gradient Descent and Adam.\n",
    "\n",
    "An intuitive illustration of the matrix factorization model is shown below:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\t<img src=\"img/rec-mf.svg\" width=\"500\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
